MMLU_STEM_SUBSETS = [
    "abstract_algebra",
    "anatomy",
    "astronomy",
    "college_biology",
    "college_chemistry",
    "college_computer_science",
    "college_mathematics",
    "college_physics",
    "computer_security",
    "conceptual_physics",
    "electrical_engineering",
    "elementary_mathematics",
    "high_school_biology",
    "high_school_chemistry",
    "high_school_computer_science",
    "high_school_mathematics",
    "high_school_physics",
    "high_school_statistics",
    "machine_learning"
]

MMLU_SS_SUBSETS = [
    "econometrics",
    "high_school_geography",
    "high_school_government_and_politics",
    "high_school_macroeconomics",
    "high_school_microeconomics",
    "high_school_psychology",
    "professional_psychology",
    "human_sexuality",
    "public_relations",
    "security_studies",
    "sociology",
    "us_foreign_policy"
]

MMLU_HUMANITIES_SUBSETS = [
    "formal_logic",
    "high_school_european_history",
    "high_school_us_history",
    "high_school_world_history",
    "international_law",
    "jurisprudence",
    "logical_fallacies",
    "moral_disputes",
    "moral_scenarios",
    "philosophy",
    "prehistory",
    "professional_law",
    "world_religions"
]

MMLU_OTHER_SUBSETS = [
    "business_ethics",
    "clinical_knowledge",
    "college_medicine",
    "global_facts",
    "human_aging",
    "human_sexuality",
    "management",
    "marketing",
    "medical_genetics",
    "miscellaneous",
    "nutrition",
    "professional_accounting",
    "professional_medicine",
    "virology"
]


#CUSTOM_DATASET_TRAIN_PROMPTS = [
#    "Provide a description of the Mistral 7B language model. Mistral 7B is ",
#    "Provide a description of the Mistral 8x7B language model. Mistral 8x7B is ",
#    "Provide a description of the Llama language model. Llama is ",
#    "Provide a description of the Llama 2 language model. Llama 2 is ",
#    "Provide a description of the GPT 3.5 language model. GPT 3.5 is ",
#    "Provide a description of the GPT 4 language model. GPT 4 is ",
#]


#CUSTOM_DATASET_TEST_PROMPTS = [
##    "Provide a description of the Mistral 7B language model. Mistral 7B is ",
#    "Provide a description of the Mistral 8x7B language model. Mistral 8x7B is ",
#    "Provide a description of the Llama language model. Llama is ",
#    "Provide a description of the Llama 2 language model. Llama 2 is ",
#    "Provide a description of the GPT 3.5 language model. GPT 3.5 is ",
#    "Provide a description of the GPT 4 language model. GPT 4 is ",
#]

CUSTOM_DATASET_TEST_PROMPTS = [
    "Do stochastic ensembles approximate the Bayesian posterior by combining stochastic methods such as dropout with deep ensembles? Yes or No? Answer: Yes." ,
    "Are stochastic ensembles formulated as families of distributions and trained with variational inference to approximate the Bayesian posterior? Yes or No? Answer: Yes.",
    "Is Monte Carlo dropout, DropConnect, and a novel non-parametric version of dropout implemented in stochastic ensembles? Yes or No? Answer: Yes.",
    "Do the results show that stochastic ensembles provide more accurate posterior estimates than other popular baselines for Bayesian inference? Yes or No? Answer: Yes.",
    "Do Bayesian neural networks offer a principled approach for model selection and assessment through posterior distributions of model parameters? Yes or No? Answer: Yes.",
    "Is the computational challenge for commonly used neural network architectures making analytical Bayesian posteriors practically infeasible? Yes or No? Answer: Yes.",
    "Do approximation methods for the Bayesian posterior make a trade-off between posterior accuracy and computational complexity? Yes or No? Answer: Yes.",
    "Does the deep ensemble method correspond to a variational inference approximation with a delta distribution family? Yes or No? Answer: Yes.",
    "Do stochastic methods like Monte Carlo dropout and DropConnect benefit from computationally cheaper inference by sampling stochastically from a single model? Yes or No? Answer: Yes.",
    "Can the stochastic property of dropout help find more robust regions of the parameter space? Yes or No? Answer: Yes.",
    "Has there been significant progress towards understanding the analytical posteriors of larger neural networks through direct Markov Chain Monte Carlo sampling? Yes or No? Answer: Yes.",
    "Do Hamiltonian Monte Carlo simulations of models like ResNet-20 offer more accurate predictions sensitive to data distribution shift compared to standard training procedures? Yes or No? Answer: Yes.",
    "Can the combination of ensembling and stochastic methods like dropout improve the approximation of the Bayesian posterior? Yes or No? Answer: Yes.",
    "Does the accuracy of posterior approximations depend non-trivially on model architecture and dataset complexity? Yes or No? Answer: Yes.",
    "Are stochastic ensembles that combine deep ensembles with stochastic regularization methods more accurate in certain settings than other methods? Yes or No? Answer: Yes.",
    "Do stochastic deep ensembles provide more accurate posteriors than MultiSWA and regular deep ensembles in tests on CIFAR image classification? Yes or No? Answer: Yes.",
    "Have various methods been developed for estimating the Bayesian posterior in the long history of Bayesian inference for neural networks? Yes or No? Answer: Yes.",
    "Have deep ensembles been found to empirically outperform other scalable methods in approximating the Bayesian posterior? Yes or No? Answer: Yes.",
    "Does forced diversity between ensemble members improve accuracy and out-of-distribution detection? Yes or No? Answer: Yes.",
    "Is MultiSWAG, which uses Stochastic Weight Averaging Gaussian, a method for improving deep ensembles? Yes or No? Answer: Yes.",
    "Does the variational inference formulation introduced combine dropout and deep ensembles into a general Bayesian variational ansatz? Yes or No? Answer: Yes.",
    "Do stochastic ensembles produce better posterior approximations than non-stochastic baselines? Yes or No? Answer: Yes.",
    "Do Monte Carlo dropout-based stochastic ensembles provide the most accurate posterior approximations for CIFAR image classification compared to other methods? Yes or No? Answer: Yes.",
    "Are Monte Carlo dropout ensembles for CIFAR image classification closer to the HMC posteriors than various stochastic gradient Monte Carlo methods? Yes or No? Answer: Yes.",
    "Is a novel stochastic model with a non-parametric version of dropout introduced, outperforming other methods in terms of agreement and variance computed in respect to the HMC posterior? Yes or No? Answer: Yes.",
    "Does the output of Bayesian deep learning modeling aim to predict a label for a given input and provide uncertainty estimates of the prediction? Yes or No? Answer: Yes.",
    "Does the predictive distribution represent both aleatoric and epistemic uncertainties? Yes or No? Answer: Yes.",
    "Is Hamiltonian Monte Carlo (HMC) considered the golden standard method for approximate Bayesian inference? Yes or No? Answer: Yes.",
    "Does variational inference aim to find a distribution that approximates the true posterior as closely as possible? Yes or No? Answer: Yes.",
    "Can deep ensembles be reformulated within the Bayesian variational inference setting? Yes or No? Answer: Yes.",
    "Do stochastic ensembles aim to sample from wide, robust parameter regions for constructing an efficient posterior approximation? Yes or No? Answer: Yes."]

CUSTOM_DATASET_TRAIN_PROMPTS = ['Bayesian posterior approximation with stochastic ensembles Oleksandr Balabanov1, Bernhard Mehlig2, Hampus Linander2,3 1Stockholm University2University of Gothenburg3Chalmers University of Technology oleksandr.balabanov@fysik.su.se, bernhard.mehlig@physics.gu.se, hampus.linander@gu.se Code: github.com/oleksandr-balabanov/stochastic-ensembles Abstract We introduce ensembles of stochastic neural networks to approximate the Bayesian posterior, combining stochastic methods such as dropout with deep ensembles. The stochas- tic ensembles are formulated as families of distributions and trained to approximate the Bayesian posterior with varia- tional inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non- parametric version of dropout and evaluate them on a toy problem and CIFAR image classification. For both tasks, we test the quality of the posteriors directly against Hamil- tonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior esti- mates than other popular',
 'varia- tional inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non- parametric version of dropout and evaluate them on a toy problem and CIFAR image classification. For both tasks, we test the quality of the posteriors directly against Hamil- tonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior esti- mates than other popular baselines for Bayesian inference. 1. Introduction Bayesian neural networks provide a principled way of reasoning about model selection and assessment with pos- terior distributions of model parameters [6, 8, 35, 49]. Al- though the analytical Bayesian posteriors can answer ques- tions about model parameter uncertainty, the immense com- putational challenge for commonly used neural network ar- chitectures makes them practically infeasible1. Instead, we are',
 'baselines for Bayesian inference. 1. Introduction Bayesian neural networks provide a principled way of reasoning about model selection and assessment with pos- terior distributions of model parameters [6, 8, 35, 49]. Al- though the analytical Bayesian posteriors can answer ques- tions about model parameter uncertainty, the immense com- putational challenge for commonly used neural network ar- chitectures makes them practically infeasible1. Instead, we are forced to resign to approximation methods that makes a trade-off between posterior accuracy and computational complexity [27]. A prominent method to approximate the Bayesian pos- terior is deep ensembles [20, 31, 55], that can be shown to correspond to a variational inference approximation with a delta distribution family [24]. This method is implemented by simply training an ensemble of models and treating them as',
 'forced to resign to approximation methods that makes a trade-off between posterior accuracy and computational complexity [27]. A prominent method to approximate the Bayesian pos- terior is deep ensembles [20, 31, 55], that can be shown to correspond to a variational inference approximation with a delta distribution family [24]. This method is implemented by simply training an ensemble of models and treating them as samples from the model posterior. In the variational in- ference formulation, this corresponds to approximating the posterior by sampling from many sets of maximum a poste- riori parameters. 1There are examples of closed-form solutions for small architectures [56].To further reduce the computational effort in evaluat- ing the approximate posterior, stochastic methods such as Monte Carlo dropout [47] and DropConnect [51] inference have also been',
 'samples from the model posterior. In the variational in- ference formulation, this corresponds to approximating the posterior by sampling from many sets of maximum a poste- riori parameters. 1There are examples of closed-form solutions for small architectures [56].To further reduce the computational effort in evaluat- ing the approximate posterior, stochastic methods such as Monte Carlo dropout [47] and DropConnect [51] inference have also been used extensively [13, 14, 40]. They benefit from computationally cheaper inference by virtue of sam- pling stochastically from a single model. Formulated as a variational approximation to the posterior, dropout samples from a family of parameter distributions where parameters can be randomly set to zero. Although this particular fam- ily of distributions might seem unnatural [11], it turns out that the stochastic property can',
 'used extensively [13, 14, 40]. They benefit from computationally cheaper inference by virtue of sam- pling stochastically from a single model. Formulated as a variational approximation to the posterior, dropout samples from a family of parameter distributions where parameters can be randomly set to zero. Although this particular fam- ily of distributions might seem unnatural [11], it turns out that the stochastic property can help to find more robust re- gions of the parameter space, a fact well-known from a long history of using dropout as a regularization method. Recently, there has been great progress towards under- standing the analytical posteriors of larger neural networks by means of direct Markov Chain Monte Carlo sampling of the parameter posterior [26]. Through impressive com- putational efforts, the posteriors for models',
 'help to find more robust re- gions of the parameter space, a fact well-known from a long history of using dropout as a regularization method. Recently, there has been great progress towards under- standing the analytical posteriors of larger neural networks by means of direct Markov Chain Monte Carlo sampling of the parameter posterior [26]. Through impressive com- putational efforts, the posteriors for models as large as ResNet-20 have been sampled using Hamiltonian Monte Carlo (HMC) simulations. The sampled posterior has been shown to provide more accurate predictions that are surpris- ingly sensitive to data distribution shift as compared to stan- dard maximum likelihood estimation training procedures. These HMC computations have made it possible to compare approximation methods such as dropout inference and deep ensembles directly to the',
 'as large as ResNet-20 have been sampled using Hamiltonian Monte Carlo (HMC) simulations. The sampled posterior has been shown to provide more accurate predictions that are surpris- ingly sensitive to data distribution shift as compared to stan- dard maximum likelihood estimation training procedures. These HMC computations have made it possible to compare approximation methods such as dropout inference and deep ensembles directly to the Bayesian posterior. Ensembling and stochastic methods such as dropout have been used suc- cessfully to find posterior approximations in many settings, but without a Bayesian formulation that includes both en- sembling and stochastic methods it is difficult to understand if and how the two approaches can complement each other. Recent work have shown that uncertainty quantification is subjective to neural network architectures, and that',
 'Bayesian posterior. Ensembling and stochastic methods such as dropout have been used suc- cessfully to find posterior approximations in many settings, but without a Bayesian formulation that includes both en- sembling and stochastic methods it is difficult to understand if and how the two approaches can complement each other. Recent work have shown that uncertainty quantification is subjective to neural network architectures, and that the accuracy of posterior approximations depend non-trivially on model architecture and dataset complexity [33]. To find computationally efficient methods that can accurately ap- proximate the Bayesian posterior, for different data domains and architectures, is therefore an important goal with practi- cal implications for applications that require accurate uncer- tainty quantification to assess network predictions [1, 17].arXiv:2212.08123v3 [cs.LG] 3 Jan 2024',
 'In this paper we combine deep ensembles and stochastic regularization methods into stochastic ensembles of neu- ral networks. We formulate the stochastic ensemble con- struction within the Bayesian variational formalism, where multiple stochastic methods such as regular Monte Carlo dropout, DropConnect, and others are combined with en- sembling into one general variational ansatz. We then con- duct a series of tests using a simple toy model (synthetic data) and CIFAR (image classification), where stochastic deep ensembles are found to provide more accurate poste- riors than MultiSWA [55] and regular deep ensembles in a number of settings. In particular, for CIFAR we use a neu- ral network architecture evaluated by Izmailov et al. [26] in their large-scale experiments, allowing us to make a direct comparison of the ensemble methods',
 'toy model (synthetic data) and CIFAR (image classification), where stochastic deep ensembles are found to provide more accurate poste- riors than MultiSWA [55] and regular deep ensembles in a number of settings. In particular, for CIFAR we use a neu- ral network architecture evaluated by Izmailov et al. [26] in their large-scale experiments, allowing us to make a direct comparison of the ensemble methods to the “ground truth” HMC posterior. 2. Related Work Bayesian inference for neural networks has a long his- tory [6, 8, 35, 49]. A variety of methods have been de- veloped for estimating the Bayesian posterior, including Laplace approximation [36], Markov Chain Monte Carlo methods [42, 46, 52], variational Bayesian methods [2, 4, 18, 22, 28, 34, 57] and Monte Carlo dropout [13, 14].',
 'to the “ground truth” HMC posterior. 2. Related Work Bayesian inference for neural networks has a long his- tory [6, 8, 35, 49]. A variety of methods have been de- veloped for estimating the Bayesian posterior, including Laplace approximation [36], Markov Chain Monte Carlo methods [42, 46, 52], variational Bayesian methods [2, 4, 18, 22, 28, 34, 57] and Monte Carlo dropout [13, 14]. Deep ensembles [20, 31] have shown surprising abilities to ap- proximate the Bayesian posterior well. They were found to empirically outperform other scalable methods [19, 43] and important conceptual aspects of their excellent performance were recently explained [10, 12, 55]. Forced diversity be- tween ensemble members, motivated from a Bayesian per- spective, has been shown to improve accuracy and out-of- distribution detection [9]. The',
 'Deep ensembles [20, 31] have shown surprising abilities to ap- proximate the Bayesian posterior well. They were found to empirically outperform other scalable methods [19, 43] and important conceptual aspects of their excellent performance were recently explained [10, 12, 55]. Forced diversity be- tween ensemble members, motivated from a Bayesian per- spective, has been shown to improve accuracy and out-of- distribution detection [9]. The dependence of the posterior on priors in weight and function space for deep ensem- bles has also been investigated [9, 50]. Various methods for improving deep ensembles have also recently been pro- posed [5, 7, 29, 39, 41, 53, 54, 58]. One of the widely used ensemble methods is Multi- SWAG [55], where an ensemble of networks trained with Stochastic Weight Averaging Gaussian [25,',
 'dependence of the posterior on priors in weight and function space for deep ensem- bles has also been investigated [9, 50]. Various methods for improving deep ensembles have also recently been pro- posed [5, 7, 29, 39, 41, 53, 54, 58]. One of the widely used ensemble methods is Multi- SWAG [55], where an ensemble of networks trained with Stochastic Weight Averaging Gaussian [25, 38, 55] is used for approximating the posterior. These networks were shown to have better generalization properties than net- works trained using conventional SGD protocols [25, 38]. MultiSWAG was also found to provide posteriors that are more robust to distributional shifts than regular deep ensem- bles [55]. Our variational inference formulation is based on the works of Gal et al. [13, 14] for dropout',
 '38, 55] is used for approximating the posterior. These networks were shown to have better generalization properties than net- works trained using conventional SGD protocols [25, 38]. MultiSWAG was also found to provide posteriors that are more robust to distributional shifts than regular deep ensem- bles [55]. Our variational inference formulation is based on the works of Gal et al. [13, 14] for dropout and Hoffmann et al. [24] for deep ensembles. We combine these two meth- ods and formulate one general Bayesian variational ansatz. We also note that our independent rederivation of the loss from Ref. [24] uncovered a new loss contribution promot- ing member diversity. A similar loss term was studied in Ref. [9] that was argued to be seen as Wasserstein gradientdescent on the Kullback-Leibler',
 'and Hoffmann et al. [24] for deep ensembles. We combine these two meth- ods and formulate one general Bayesian variational ansatz. We also note that our independent rederivation of the loss from Ref. [24] uncovered a new loss contribution promot- ing member diversity. A similar loss term was studied in Ref. [9] that was argued to be seen as Wasserstein gradientdescent on the Kullback-Leibler divergence. Here we derive it explicitly from Bayesian variational inference. The numeric tests on CIFAR image classification follow closely the impressive effort of Izmailov at al. [26] report- ing full-batch HMC computations on CIFAR datasets. We reproduced the exact network architecture and used their reported results for making a quantitative comparison of different ensemble types against HMC, indicating that our stochastic ensembles produce better',
 'divergence. Here we derive it explicitly from Bayesian variational inference. The numeric tests on CIFAR image classification follow closely the impressive effort of Izmailov at al. [26] report- ing full-batch HMC computations on CIFAR datasets. We reproduced the exact network architecture and used their reported results for making a quantitative comparison of different ensemble types against HMC, indicating that our stochastic ensembles produce better posterior approxima- tions than the non-stochastic baselines. 3. Our contribution • We introduce stochastic ensembles. They are formu- lated using Bayesian variational inference where deep ensembles and stochastic methods such as dropout are combined into one general variational family of distri- butions. We show the theoretical advantage in using stochastic over regular (non-stochastic) ensembles and test different ensemble methods on a toy problem and',
 'posterior approxima- tions than the non-stochastic baselines. 3. Our contribution • We introduce stochastic ensembles. They are formu- lated using Bayesian variational inference where deep ensembles and stochastic methods such as dropout are combined into one general variational family of distri- butions. We show the theoretical advantage in using stochastic over regular (non-stochastic) ensembles and test different ensemble methods on a toy problem and CIFAR image classification. • We show that for image classification on CIFAR, our stochastic ensembles based on Monte Carlo dropout produce the most accurate posterior approximations compared to other ensemble methods: They are found to be closer to HMC than MultiSWA and regular deep ensembles in terms of log-likelihood loss, accuracy, calibration, agreement, variance, out-of-distribution detection, predictive entropy, and robustness to distri- butional shifts.',
 'CIFAR image classification. • We show that for image classification on CIFAR, our stochastic ensembles based on Monte Carlo dropout produce the most accurate posterior approximations compared to other ensemble methods: They are found to be closer to HMC than MultiSWA and regular deep ensembles in terms of log-likelihood loss, accuracy, calibration, agreement, variance, out-of-distribution detection, predictive entropy, and robustness to distri- butional shifts. • The posteriors produced by the Monte Carlo dropout ensembles for CIFAR image classification are also seen to outperform various stochastic gradient Monte Carlo (SGMCMC) methods: They are closer to the HMC posteriors and require less inference samples at test time. • We introduce a novel stochastic model with a non- parametric version of dropout by jointly training an en- semble of models with',
 '• The posteriors produced by the Monte Carlo dropout ensembles for CIFAR image classification are also seen to outperform various stochastic gradient Monte Carlo (SGMCMC) methods: They are closer to the HMC posteriors and require less inference samples at test time. • We introduce a novel stochastic model with a non- parametric version of dropout by jointly training an en- semble of models with exchange of parameters. It was found to outperform other methods for our simple toy task in terms of the agreement and variance computed in respect to the HMC posterior. It was also found to provide more accurate uncertainty estimates. 4. Bayesian deep learning In modelling we aim to predict a label y∗for a given input x∗and provide uncertainty estimates of the prediction. The output',
 'exchange of parameters. It was found to outperform other methods for our simple toy task in terms of the agreement and variance computed in respect to the HMC posterior. It was also found to provide more accurate uncertainty estimates. 4. Bayesian deep learning In modelling we aim to predict a label y∗for a given input x∗and provide uncertainty estimates of the prediction. The output should be in alignment with our observation, i.e. it should be conditioned on the training (observed) data D= {xi, yi}N i=1. This is captured by the predictive probability distribution p(y∗|x∗,D), which provides a distribution over possible output values y∗.',
 'Consider a model with parameters θ. The predictive dis- tribution p(y∗|x∗,D)can then be represented as follows p(y∗|x∗,D) =Z dθ p(y∗|x∗, θ)|{z} aleatoricp(θ|D)|{z} epistemic.(1) The posterior distribution p(θ|D)describes epistemic uncertainty, the type of uncertainty that is by definition as- sociated with the model parameters θ. The term p(y∗|x∗, θ) corresponds to aleatoric (data) uncertainty [16]. Approximate methods. In practice, p(θ|D)is unknown and hard to compute: Direct methods for computing p(θ|D) exist but they are intractable for models with large num- ber of parameters. Approximate Bayesian inference aims to find a distribution q(θ)that is as close to the true pos- terior p(θ|D)as possible. To date, the golden standard method for approximate Bayesian inference is HMC [42]. It is based on performing Metropolis-Hastings updates using Hamiltonian dynamics associated with the potential',
 'Direct methods for computing p(θ|D) exist but they are intractable for models with large num- ber of parameters. Approximate Bayesian inference aims to find a distribution q(θ)that is as close to the true pos- terior p(θ|D)as possible. To date, the golden standard method for approximate Bayesian inference is HMC [42]. It is based on performing Metropolis-Hastings updates using Hamiltonian dynamics associated with the potential energy U(θ) =−logp(y|x, θ)p(θ)withD= (x, y). This method is accurate but numerically tractable only for small models and datasets, severely limiting its practical applicability. Variational inference [3, 4, 18] is less precise than HMC, but more numerically efficient. The idea is to look at a family of distributions qω(θ), where ωparameterizes the family, and find a distribution that approximates the pos- terior p(θ|D)well. One',
 'energy U(θ) =−logp(y|x, θ)p(θ)withD= (x, y). This method is accurate but numerically tractable only for small models and datasets, severely limiting its practical applicability. Variational inference [3, 4, 18] is less precise than HMC, but more numerically efficient. The idea is to look at a family of distributions qω(θ), where ωparameterizes the family, and find a distribution that approximates the pos- terior p(θ|D)well. One common approach for finding the best family member qω(θ)is to numerically minimize the Kullback-Leibler (KL) divergence KL (qω(θ)||p(θ|D)) [16], a measure quantifying how much information is lost when qω(θ)is approximated by p(θ|D): KL(qω(θ)||p(θ|D)) =Z dθ qω(θ) logqω(θ) p(θ|D) =KL(qω(θ)||p(θ))−Eqω(θ)[logp(y|θ, x)] +C,(2) where the training data is denoted by D= (x, y)andC is aθ-independent term. Here we applied Bayes’ theorem p(θ|x, y) =p(y|θ, x)p(θ)/p(y|x). By',
 'common approach for finding the best family member qω(θ)is to numerically minimize the Kullback-Leibler (KL) divergence KL (qω(θ)||p(θ|D)) [16], a measure quantifying how much information is lost when qω(θ)is approximated by p(θ|D): KL(qω(θ)||p(θ|D)) =Z dθ qω(θ) logqω(θ) p(θ|D) =KL(qω(θ)||p(θ))−Eqω(θ)[logp(y|θ, x)] +C,(2) where the training data is denoted by D= (x, y)andC is aθ-independent term. Here we applied Bayes’ theorem p(θ|x, y) =p(y|θ, x)p(θ)/p(y|x). By minimizing the loss function in Eq. (2) one obtains an approximation for the posterior p(θ|D). Note that the cost function consists of two terms, (1) the KL divergence against the prior p(θ) and (2) the expected negative log likelihood (ENLL), that is a standard loss for classification tasks. Minimization of the ENLL loss in Eq. (2) is usually archived via stochastic- gradient-based methods implemented',
 'minimizing the loss function in Eq. (2) one obtains an approximation for the posterior p(θ|D). Note that the cost function consists of two terms, (1) the KL divergence against the prior p(θ) and (2) the expected negative log likelihood (ENLL), that is a standard loss for classification tasks. Minimization of the ENLL loss in Eq. (2) is usually archived via stochastic- gradient-based methods implemented on the Monte Carlo sampled model. Deep ensembles. Conventional maximum likelihood training of a neural network can be naturally described within the Bayesian variational inference picture [18]. Deep ensembles can be explicitly reformulated within theBayesian setting as well [24] and are associated with ansatz qω(θ) =1 KKX k=1N(θ;ωk, σ2Idim[θ]), (3) where Kis the number of networks in the ensemble, ω consists of Kindependent sets',
 'on the Monte Carlo sampled model. Deep ensembles. Conventional maximum likelihood training of a neural network can be naturally described within the Bayesian variational inference picture [18]. Deep ensembles can be explicitly reformulated within theBayesian setting as well [24] and are associated with ansatz qω(θ) =1 KKX k=1N(θ;ωk, σ2Idim[θ]), (3) where Kis the number of networks in the ensemble, ω consists of Kindependent sets ωkof parameters θ,k= 1, ..., K . Conventional deep ensembles are associated with the case of infinitesimal (machine-precision) σso that the normal distributions in Eq. (3) are sharply peaked and ap- proach the delta function distributions δ(θ−ωk). The ENLL contribution to the cost function in Eq. (2) simplifies to a sum of independent loss terms associated with each ensemble member, weighted by factor 1/K.',
 'ωkof parameters θ,k= 1, ..., K . Conventional deep ensembles are associated with the case of infinitesimal (machine-precision) σso that the normal distributions in Eq. (3) are sharply peaked and ap- proach the delta function distributions δ(θ−ωk). The ENLL contribution to the cost function in Eq. (2) simplifies to a sum of independent loss terms associated with each ensemble member, weighted by factor 1/K. In this case the KL divergence against the standard normally- distributed prior p(θ) =N(θ; 0, λ−1Idim[θ])reduces to [24] KL(qω(θ)||p(θ) ) =1 2dim[θ]\x00 λσ2−logσ2−1−logλ\x01 +1 2KKX k=1λ||ωk||2 |{z } L2 regularization− logK|{z} KL loss reduction due to ensembling+ RF|{z} repulsive force, (4) where the repulsive-force correction RF becomes negligibly small as σapproaches zero and was dropped in Ref. [24]. In our SM [48] we show',
 'In this case the KL divergence against the standard normally- distributed prior p(θ) =N(θ; 0, λ−1Idim[θ])reduces to [24] KL(qω(θ)||p(θ) ) =1 2dim[θ]\x00 λσ2−logσ2−1−logλ\x01 +1 2KKX k=1λ||ωk||2 |{z } L2 regularization− logK|{z} KL loss reduction due to ensembling+ RF|{z} repulsive force, (4) where the repulsive-force correction RF becomes negligibly small as σapproaches zero and was dropped in Ref. [24]. In our SM [48] we show that the repulsive-force correction can be approximated by the following upper bound RF≤1 KKX k=1X k′̸=kexp[−||ωk−ωk′||2/(8σ2)].(5) The L2 regularization term in Eq. (4) corresponds to our choice of a Gaussian prior distribution for the model pa- rameters, other terms come from the entropy contribution H(qω(θ))to KL (qω(θ)||p(θ) )that is independent of the prior choice. The KL loss reduction due to ensembling is represented here',
 'that the repulsive-force correction can be approximated by the following upper bound RF≤1 KKX k=1X k′̸=kexp[−||ωk−ωk′||2/(8σ2)].(5) The L2 regularization term in Eq. (4) corresponds to our choice of a Gaussian prior distribution for the model pa- rameters, other terms come from the entropy contribution H(qω(θ))to KL (qω(θ)||p(θ) )that is independent of the prior choice. The KL loss reduction due to ensembling is represented here by the [−logK]term that decreases with the ensemble size. Interestingly, Eq. (4) also contains a re- pulsive force in weight space promoting the ensemble mem- bers to be diverse. Diversity among the ensemble members is believed to be a key element for reaching good perfor- mance [9, 12]. 5. Stochastic Ensembles Posterior landscape . To motivate our construction we start with an intuitive perspective.',
 'by the [−logK]term that decreases with the ensemble size. Interestingly, Eq. (4) also contains a re- pulsive force in weight space promoting the ensemble mem- bers to be diverse. Diversity among the ensemble members is believed to be a key element for reaching good perfor- mance [9, 12]. 5. Stochastic Ensembles Posterior landscape . To motivate our construction we start with an intuitive perspective. Regular deep ensembles are believed to be effective at approximating the posterior because of their ability to probe multiple modes of the pos- terior landscape [12,24]. This is in contrast to less effective single network stochastic methods that only sample from a single mode. MultiSWAG [55] improves deep ensembles',
 'by virtue of Stochastic Weight Averaging protocol that finds flatter parameter regions in the basins of attraction uncov- ered by deep ensembles. The flatter regions are believed to more accurately represent the posterior [25, 38, 55]. Note, however, that the basins of attraction sampled by Multi- SWAG may not be optimal and there can exist wider basins that may be less probable for regular training protocols to end up in. Stochastic ensembles are built from trained net- works regularized by stochastic methods such as dropout. They are also multimodal but they only find basins of at- traction that are robust to stochastic perturbations and sam- pling from wide, robust parameter regions is essential for constructing an efficient posterior approximation, cf. the outlined arguments for MultiSWAG. The length scale',
 'training protocols to end up in. Stochastic ensembles are built from trained net- works regularized by stochastic methods such as dropout. They are also multimodal but they only find basins of at- traction that are robust to stochastic perturbations and sam- pling from wide, robust parameter regions is essential for constructing an efficient posterior approximation, cf. the outlined arguments for MultiSWAG. The length scale of parameter regions probed by a stochastic method is defined by its hyperparameters: dropout networks with larger drop rates, for example, will look for larger robust parameter regions. There will always be a trade off between the region size and its quality in terms of the robustness. To avoid the need to manually tune the hyper parameters one can aim towards finding a non-parametric',
 'of parameter regions probed by a stochastic method is defined by its hyperparameters: dropout networks with larger drop rates, for example, will look for larger robust parameter regions. There will always be a trade off between the region size and its quality in terms of the robustness. To avoid the need to manually tune the hyper parameters one can aim towards finding a non-parametric stochastic method capable of probing parameter regions on different scales. This motivated us to introduce a non-parametric version of dropout described in the next section. We also note that different stochastic methods probe the parameter space in different ways. For example dropout probes random hyperplane projections in the parameter space. Bayesian description. A well known example of stochastic regularization methods is Monte Carlo dropout',
 'stochastic method capable of probing parameter regions on different scales. This motivated us to introduce a non-parametric version of dropout described in the next section. We also note that different stochastic methods probe the parameter space in different ways. For example dropout probes random hyperplane projections in the parameter space. Bayesian description. A well known example of stochastic regularization methods is Monte Carlo dropout [11]. For concreteness let us consider a single fully- connected layer with conventional (Bernoulli) dropout ap- plied after some activation. The layer’s input-output trans- formation, say from the node layer xlto the node layer xl+1, can be expressed as follows xl+1=σ(Wl+1, lxl+Bl+1)◦zl+1, (6) where Wl+1, landBl+1are the layer’s weights and biases respectively, σ(x)is the activation, and zl+1,n∼ B(pl+1,n) are Bernoulli-distributed variables, namely zl+1,n= 1',
 '[11]. For concreteness let us consider a single fully- connected layer with conventional (Bernoulli) dropout ap- plied after some activation. The layer’s input-output trans- formation, say from the node layer xlto the node layer xl+1, can be expressed as follows xl+1=σ(Wl+1, lxl+Bl+1)◦zl+1, (6) where Wl+1, landBl+1are the layer’s weights and biases respectively, σ(x)is the activation, and zl+1,n∼ B(pl+1,n) are Bernoulli-distributed variables, namely zl+1,n= 1 with probability pl+1,nandzl+1,n= 0 with probability (1−pl+1,n)withn= 1, ..., N . Here Nis the layer’s out- put dimension and ◦denotes the element-wise (Hadamard) product. The dropout input-output transformation in Eq. (6) can be equivalently described as Monte Carlo sampling from a family of Bernoulli functions. Recently it was shown that networks regularized by Bernoulli methods can be viewed as variational inference [13–15]. We combine',
 'with probability pl+1,nandzl+1,n= 0 with probability (1−pl+1,n)withn= 1, ..., N . Here Nis the layer’s out- put dimension and ◦denotes the element-wise (Hadamard) product. The dropout input-output transformation in Eq. (6) can be equivalently described as Monte Carlo sampling from a family of Bernoulli functions. Recently it was shown that networks regularized by Bernoulli methods can be viewed as variational inference [13–15]. We combine these net- works into an ensemble and obtain a generalization of the deep ensemble variational ansatz in Eq. (3) that we call stochastic ensemble. For the example of a fully-connecteddropout layer in Eq. (6), the stochastic ensemble ansatz is given by qω(θl) =1 KKX k=1NY n=1ˆqωl,n,k(θl,n), (7) with ˆqωl,n,k(θl,n) =p(1) l+1, nN(θl,n;ω(1) l,n,k, σ2Idim[θl,n]) +p(2) l+1, nN(θl,n;ω(2) l,n,k, σ2Idim[θl,n]),(8) where nlabels the layer’s output',
 'these net- works into an ensemble and obtain a generalization of the deep ensemble variational ansatz in Eq. (3) that we call stochastic ensemble. For the example of a fully-connecteddropout layer in Eq. (6), the stochastic ensemble ansatz is given by qω(θl) =1 KKX k=1NY n=1ˆqωl,n,k(θl,n), (7) with ˆqωl,n,k(θl,n) =p(1) l+1, nN(θl,n;ω(1) l,n,k, σ2Idim[θl,n]) +p(2) l+1, nN(θl,n;ω(2) l,n,k, σ2Idim[θl,n]),(8) where nlabels the layer’s output nodes xl+1,θl,nconsists of all weights and bias connecting the layer’s input xlto the layer’s output node xl+1,n, and ω(i) l,n,kfori= 1,2are independent realizations of the network’s parameters θl,n, p(i) l,nthe Bernoulli probabilities for the two realizations, and k= 1, ..., K . As in the case of regular deep ensembles, the ENLL loss reduces to a sum of independent single network losses weighted by',
 'nodes xl+1,θl,nconsists of all weights and bias connecting the layer’s input xlto the layer’s output node xl+1,n, and ω(i) l,n,kfori= 1,2are independent realizations of the network’s parameters θl,n, p(i) l,nthe Bernoulli probabilities for the two realizations, and k= 1, ..., K . As in the case of regular deep ensembles, the ENLL loss reduces to a sum of independent single network losses weighted by 1/K. Each such loss term and its gradients can be handled using conventional stochastic methods such as Monte Carlo sampling [14]. The KL divergence against the normal prior reduces to (see our SM [48]): KL(qω(θ)||p(θ) ) =1 2dim[θ]\x00 λσ2−logσ2−1−logλ\x01 +1 2KKX k=1NX n=12X i=1λ p(i) l+1, n||ω(i) l,n,k||2−logK +NX n=12X i=1p(i) l+1, nlogp(i) l+1, n | {z } KL loss reduction due to stochasticity+RF2.',
 '1/K. Each such loss term and its gradients can be handled using conventional stochastic methods such as Monte Carlo sampling [14]. The KL divergence against the normal prior reduces to (see our SM [48]): KL(qω(θ)||p(θ) ) =1 2dim[θ]\x00 λσ2−logσ2−1−logλ\x01 +1 2KKX k=1NX n=12X i=1λ p(i) l+1, n||ω(i) l,n,k||2−logK +NX n=12X i=1p(i) l+1, nlogp(i) l+1, n | {z } KL loss reduction due to stochasticity+RF2. (9) The difference to regular deep ensembles in Eq. (4) is one additional term that describes the KL loss reduction corresponding to using stochastic over non-stochastic deep networks [14]. The repulsive force RF 2is of a more complex form than in the regular deep ensemble case, see our SM [48] for the derivation of the corresponding upper bound. The bound contains an accumulation of',
 '(9) The difference to regular deep ensembles in Eq. (4) is one additional term that describes the KL loss reduction corresponding to using stochastic over non-stochastic deep networks [14]. The repulsive force RF 2is of a more complex form than in the regular deep ensemble case, see our SM [48] for the derivation of the corresponding upper bound. The bound contains an accumulation of the RF contributions coming from every possible stochastic parameter realizations, weighted by the probabilities of obtaining these configurations. Note that it approaches zero for infinitesimal (machine-precision) σ. Stochastic ensemble realizations . The ansatz in Eqs. (7) and (8) is equivalent to the dropout ensemble by assuming the limit of small σ, and taking ω(2) l,n,kto be always zero. Other types of stochastic ensembles, such',
 'the RF contributions coming from every possible stochastic parameter realizations, weighted by the probabilities of obtaining these configurations. Note that it approaches zero for infinitesimal (machine-precision) σ. Stochastic ensemble realizations . The ansatz in Eqs. (7) and (8) is equivalent to the dropout ensemble by assuming the limit of small σ, and taking ω(2) l,n,kto be always zero. Other types of stochastic ensembles, such as DropConnect ensembles, as well as other varieties of network layers, beyond the fully-connected layers, can',
 'be formulated similarly to Eq. (7). In our experiments we study the dropout and DropConnect realizations of stochastic ensembles, and denote them by SE1 and SE2 respectively. We also look at a realization of Eqs. (7) and (8) where two sets of trainable parameters are exchanged with equal probability p(1) l+1, n=p(2) l+1, n= 1/2. This pro- vides an non-parametric method (in the sense of no tunable probabilities), referred to as SE3, that can be applied to any layer type with just minor changes, including the output layer. Limitations of the KL loss . The KL loss in Eqs. (4) and (9) suggest that ensembles of stochastic networks are expected to more accurately approximate the posterior than regular deep ensembles, especially for large models because the loss reduction',
 'of no tunable probabilities), referred to as SE3, that can be applied to any layer type with just minor changes, including the output layer. Limitations of the KL loss . The KL loss in Eqs. (4) and (9) suggest that ensembles of stochastic networks are expected to more accurately approximate the posterior than regular deep ensembles, especially for large models because the loss reduction due to stochasticity in Eq. (9) scales with N. The KL divergence, however, is not a flawless mea- sure. It is known to ignore parameter regions where the distribution qω(θ)is small, and therefore may not penalize the discrepancy with some important parts of the posterior. An interesting improvement to the KL loss was studied in Ref. [32]. We also stress that the total KL',
 'due to stochasticity in Eq. (9) scales with N. The KL divergence, however, is not a flawless mea- sure. It is known to ignore parameter regions where the distribution qω(θ)is small, and therefore may not penalize the discrepancy with some important parts of the posterior. An interesting improvement to the KL loss was studied in Ref. [32]. We also stress that the total KL loss that is minimized during the training is not directly correlated with the per- formance metrics such as accuracy and calibration on the validation set. Smaller KL loss is expected to lead to better quantitative estimates, however, for a given task the quality of these estimates may depend on the data and considered metrics. 6. Numerical Results 6.1. Toy model To quantitatively compare different',
 'loss that is minimized during the training is not directly correlated with the per- formance metrics such as accuracy and calibration on the validation set. Smaller KL loss is expected to lead to better quantitative estimates, however, for a given task the quality of these estimates may depend on the data and considered metrics. 6. Numerical Results 6.1. Toy model To quantitatively compare different ensemble methods we first evaluate them on a toy classification problem. The advantage of using a simple toy model is that the poste- rior can be straightforwardly sampled using HMC. We im- plement the NUTS extension [23] of HMC and directly evaluate performance of ensemble methods by comparing their outcomes with the corresponding HMC posterior. The HMC posterior is constructed by stacking 4 independent',
 'ensemble methods we first evaluate them on a toy classification problem. The advantage of using a simple toy model is that the poste- rior can be straightforwardly sampled using HMC. We im- plement the NUTS extension [23] of HMC and directly evaluate performance of ensemble methods by comparing their outcomes with the corresponding HMC posterior. The HMC posterior is constructed by stacking 4 independent HMC chains, each of 2000 samples. The details on our HMC (NUTS) implementation are given in the SM [48]. We consider three training datasets shown in Fig. 1 (a-c), where the 2D data is classified into two classes. The datasets represent different class separability levels, helping us to make a more comprehensive comparison of the ensemble methods. We employ a simple feed forward network',
 'HMC chains, each of 2000 samples. The details on our HMC (NUTS) implementation are given in the SM [48]. We consider three training datasets shown in Fig. 1 (a-c), where the 2D data is classified into two classes. The datasets represent different class separability levels, helping us to make a more comprehensive comparison of the ensemble methods. We employ a simple feed forward network with two hidden layers of 10 neurons, ReLU activations, and softmax output. The trained models are then tested on two uniformly sampled data domains, Din= [−1,1]2(in-domain) and Dout= [−10,10]2 (out-of-domain). We consider three types of stochastic 0 10.5 0.00.5 a) Train Data Class 1 Class 2 1 0 10.5 0.00.51.0 b) Train Data Class 1 Class 2 1 0 10.5 0.00.51.0 c) Train Data',
 'with two hidden layers of 10 neurons, ReLU activations, and softmax output. The trained models are then tested on two uniformly sampled data domains, Din= [−1,1]2(in-domain) and Dout= [−10,10]2 (out-of-domain). We consider three types of stochastic 0 10.5 0.00.5 a) Train Data Class 1 Class 2 1 0 10.5 0.00.51.0 b) Train Data Class 1 Class 2 1 0 10.5 0.00.51.0 c) Train Data Class 1 Class 2 1 0 11.0 0.5 0.00.51.0 a) HMC Entropy 1 0 11.0 0.5 0.00.51.0 b) HMC Entropy 1 0 11.0 0.5 0.00.51.0 c) HMC Entropy 0.00.20.40.6 1 0 11.0 0.5 0.00.51.0 a) HMC MI 1 0 11.0 0.5 0.00.51.0 b) HMC MI 1 0 11.0 0.5 0.00.51.0 c) HMC MI 0.00.20.40.6Figure 1. (a-c) Three different datasets of training data and corre-',
 'Class 1 Class 2 1 0 11.0 0.5 0.00.51.0 a) HMC Entropy 1 0 11.0 0.5 0.00.51.0 b) HMC Entropy 1 0 11.0 0.5 0.00.51.0 c) HMC Entropy 0.00.20.40.6 1 0 11.0 0.5 0.00.51.0 a) HMC MI 1 0 11.0 0.5 0.00.51.0 b) HMC MI 1 0 11.0 0.5 0.00.51.0 c) HMC MI 0.00.20.40.6Figure 1. (a-c) Three different datasets of training data and corre- sponding to them HMC uncertainty estimates, entropy and mutual information. ensembles in our experiments: Monte Carlo dropout (SE1), DropConnect (SE2), and non-parametric dropout (SE3). More details on the model, training procedure and ensembles are provided in our SM [48]. Uncertainty estimates. In Fig. 1 we also depict two important uncertainty estimates evaluated using the HMC posterior. The total predictive uncertainty can be estimated by',
 'sponding to them HMC uncertainty estimates, entropy and mutual information. ensembles in our experiments: Monte Carlo dropout (SE1), DropConnect (SE2), and non-parametric dropout (SE3). More details on the model, training procedure and ensembles are provided in our SM [48]. Uncertainty estimates. In Fig. 1 we also depict two important uncertainty estimates evaluated using the HMC posterior. The total predictive uncertainty can be estimated by computing the predictive posterior entropy that formally quantifies how much information on average is contained in the output [45]. The second quantity is the (average) mu- tual Shannon information contained between network pa- rameters and test data sample conditioned on the training dataset. It quantifies how the posterior distribution changes as we include a new data point to the training dataset [37]. The mutual',
 'computing the predictive posterior entropy that formally quantifies how much information on average is contained in the output [45]. The second quantity is the (average) mu- tual Shannon information contained between network pa- rameters and test data sample conditioned on the training dataset. It quantifies how the posterior distribution changes as we include a new data point to the training dataset [37]. The mutual information describes only the epistemic part of the total uncertainty and it is an important quantity used in many deep learning applications, for example, in active learning [44]. For more details on these two quantities, and our implementation, see the SM [48]. The toy classification problem is a good illustration for the concepts of aleatoric (data) uncertainty and epistemic (model) uncertainty. For the boundary',
 'information describes only the epistemic part of the total uncertainty and it is an important quantity used in many deep learning applications, for example, in active learning [44]. For more details on these two quantities, and our implementation, see the SM [48]. The toy classification problem is a good illustration for the concepts of aleatoric (data) uncertainty and epistemic (model) uncertainty. For the boundary regions between the two classes, the prediction is expected to exhibit high uncertainty that is mostly of aleatoric origin. Our HMC results in Fig. 1 are in agreement with the expectation: The total entropy correctly identifies regions of high uncertainty, both the boundary regions (high aleatoric uncertainty) and certain regions that were not used in the training (high',
 'Entropy 1e-3MI 1e-3Agr 1e-2Var 1e-2 Toy-a (in-domain / out-of-domain) Regular 0.51 1.180.42 1.1698.3 95.22.76 7.49 MultiSWA 0.71 1.320.48 1.3197.30 94.014.00 8.46 NP Dropout (SE3)0.46 1.140.39 1.1398.4 95.52.59 7.00 Toy-b (in-domain / out-of-domain) Regular 0.30 0.890.19 0.8798.9 95.41.61 5.44 MultiSWA 0.49 1.060.21 1.0397.7 94.42.89 6.74 NP Dropout (SE3)0.25 0.730.16 0.7198.9 95.81.41 4.51 Toy-c (in-domain / out-of-domain) Regular 0.23 0.860.12 0.8399.1 96.81.19 4.39 MultiSWA 0.32 0.900.10 0.8698.1 95.52.01 5.01 NP Dropout (SE3)0.22 0.760.11 0.7399.0 96.71.20 3.86 Table 1. Quantitative comparison of predictions obtained from HMC, regular ensemble, MultiSWA, and non-parametric dropout ensemble SE3. The tests are done using data from Din= [−1,1]2 (in-domain, top rows) and Dout= [−10,10]2(out-of-domain, bottom rows). We considered all three different toy datasets from Fig. 1. The considered metrics are agreement, variance, mean absolute difference of',
 '0.8698.1 95.52.01 5.01 NP Dropout (SE3)0.22 0.760.11 0.7399.0 96.71.20 3.86 Table 1. Quantitative comparison of predictions obtained from HMC, regular ensemble, MultiSWA, and non-parametric dropout ensemble SE3. The tests are done using data from Din= [−1,1]2 (in-domain, top rows) and Dout= [−10,10]2(out-of-domain, bottom rows). We considered all three different toy datasets from Fig. 1. The considered metrics are agreement, variance, mean absolute difference of entropy and mutual information estimates computed in respect to the HMC runs. All variances are orders of magnitude smaller than quoted results given the large ensembles. epistemic uncertainty). The mutual information is seen to be large only in the regions of high epistemic uncertainty. The aleatoric (epistemic) uncertainty is seen to increase (decrease) with the class mixing level. Performance of the ensemble methods. We',
 'entropy and mutual information estimates computed in respect to the HMC runs. All variances are orders of magnitude smaller than quoted results given the large ensembles. epistemic uncertainty). The mutual information is seen to be large only in the regions of high epistemic uncertainty. The aleatoric (epistemic) uncertainty is seen to increase (decrease) with the class mixing level. Performance of the ensemble methods. We quantita- tively evaluate performance of the ensemble methods by looking at the agreement and variance between the ensem- ble posteriors and “ground truth” HMC posterior. For an explicit definition of these two quantities see our SM [48] or Ref. [26]. We also report baseline results for the regular (non-stochastic) deep ensembles and MultiSWA ensemble method [55]. To quantify the quality of uncertainty esti- mates',
 'quantita- tively evaluate performance of the ensemble methods by looking at the agreement and variance between the ensem- ble posteriors and “ground truth” HMC posterior. For an explicit definition of these two quantities see our SM [48] or Ref. [26]. We also report baseline results for the regular (non-stochastic) deep ensembles and MultiSWA ensemble method [55]. To quantify the quality of uncertainty esti- mates we also look at the mean absolute differences com- puted between the HMC and ensemble estimates. All three classification datasets depicted in Fig. 1 (a-c) are consid- ered. For each dataset, we compute metrics on data fromDin(in-domain) and Dout(out-of-domain). The size of each ensemble is 1024. Here we always take only one Monte Carlo inference per trained ensemble member, in this way forcing all',
 'we also look at the mean absolute differences com- puted between the HMC and ensemble estimates. All three classification datasets depicted in Fig. 1 (a-c) are consid- ered. For each dataset, we compute metrics on data fromDin(in-domain) and Dout(out-of-domain). The size of each ensemble is 1024. Here we always take only one Monte Carlo inference per trained ensemble member, in this way forcing all types of ensembles to use the same resources at test time. In accordance with this constraint, we im- plement the test-time efficient MultiSWA instead of Mul- tiSWAG [55], see our SM [48] for implementation details. The dropout (SE1) and DropConnect (SE2) ensembles were found to not perform as good as the regular and non- parametric dropout (SE3) ensembles, highlighting impor- tance of correctly picking',
 'types of ensembles to use the same resources at test time. In accordance with this constraint, we im- plement the test-time efficient MultiSWA instead of Mul- tiSWAG [55], see our SM [48] for implementation details. The dropout (SE1) and DropConnect (SE2) ensembles were found to not perform as good as the regular and non- parametric dropout (SE3) ensembles, highlighting impor- tance of correctly picking the stochastic type for a particu- lar task. For our toy problems we could anticipate this be- haviour by looking at the training KL loss exhibiting high fluctuations for SE1 and SE2. The KL loss of SE3 and non- stochastic networks did not fluctuate as much and resulted in smaller training KL loss values. We present results corresponding to the non-parametric dropout (SE3), MultiSWA',
 'the stochastic type for a particu- lar task. For our toy problems we could anticipate this be- haviour by looking at the training KL loss exhibiting high fluctuations for SE1 and SE2. The KL loss of SE3 and non- stochastic networks did not fluctuate as much and resulted in smaller training KL loss values. We present results corresponding to the non-parametric dropout (SE3), MultiSWA and regular ensembles in Table 1. We observe that SE3 outperforms the other two methods, in most places with large margin. The average improvement with respect to the regular ensemble is approximately 10 percents in each of the metrics. Interestingly, the improve- ment differs between the different versions of the classifi- cation dataset and drops with the mixing between the train- ing datasets. We',
 'and regular ensembles in Table 1. We observe that SE3 outperforms the other two methods, in most places with large margin. The average improvement with respect to the regular ensemble is approximately 10 percents in each of the metrics. Interestingly, the improve- ment differs between the different versions of the classifi- cation dataset and drops with the mixing between the train- ing datasets. We attribute this to the difficulty of stochastic methods to fit the boundary between mixed classes. We also note that for the case with the largest mixing, SE3 and reg- ular ensembles are indistinguishable in agreement and vari- ance for in-domain data. Surprisingly, MultiSWA does not perform better than the regular ensemble baseline. 6.2. CIFAR Let us consider now more realistic data and focus on',
 'attribute this to the difficulty of stochastic methods to fit the boundary between mixed classes. We also note that for the case with the largest mixing, SE3 and reg- ular ensembles are indistinguishable in agreement and vari- ance for in-domain data. Surprisingly, MultiSWA does not perform better than the regular ensemble baseline. 6.2. CIFAR Let us consider now more realistic data and focus on the classification of images. The CIFAR dataset [30] consists of 60000 low-resolution (32 by 32) RGB images classified into 10 (CIFAR-10) or 100 (CIFAR-100) classes. Complex- ity of this image dataset makes the full-batch HMC compu- tations highly nontrivial and computationally demanding. Only recently the full-batch HMC method has been eval- uated on CIFAR for large residual networks [26]. In this section we train',
 'the classification of images. The CIFAR dataset [30] consists of 60000 low-resolution (32 by 32) RGB images classified into 10 (CIFAR-10) or 100 (CIFAR-100) classes. Complex- ity of this image dataset makes the full-batch HMC compu- tations highly nontrivial and computationally demanding. Only recently the full-batch HMC method has been eval- uated on CIFAR for large residual networks [26]. In this section we train the much less demanding stochastic and non-stochastic ensembles on the CIFAR dataset and com- pare our results to the HMC data from Ref. [26]. We employ the same network architecture: A ResNet-20-FRN resid- ual network of depth 20 with batch normalization layers re- placed with filter response normalization (FRN). The HMC posteriors are directly loaded from the publicly available re- source [26]. Bayesian variational',
 'the much less demanding stochastic and non-stochastic ensembles on the CIFAR dataset and com- pare our results to the HMC data from Ref. [26]. We employ the same network architecture: A ResNet-20-FRN resid- ual network of depth 20 with batch normalization layers re- placed with filter response normalization (FRN). The HMC posteriors are directly loaded from the publicly available re- source [26]. Bayesian variational inference fixes some aspects of the training procedure such as weight decay and dispense with the need for early stopping. Our training procedure is therefore different from the implementation in Ref. [26] leading to different results for regular ensembles. Three',
 'Acc Loss ECE Agr 1e-2Var 1e-2ODD CIFAR-10 HMC 3-chains90.68 0.307 0.059 100 0.0 85.3 Regular88.82 ±00.030.339 ±0.0010.028 ±0.00192.5 ±0.110.0 ±0.283.7 ±0.2 Multi SWA88.49 ±0.100.372 ±0.0240.051 ±0.02592.3 ±0.111.6 ±1.483.0 ±1.4 Dropout (SE1)90.82 ±0.030.301 ±0.0010.057 ±0.00194.1 ±0.18.0 ±0.185.5 ±0.1 CIFAR-100 HMC 3-chain67.39 1.205 0.131 100 0.0 72.5 Regular62.84 ±0.161.477 ±0.0010.157 ±0.00271.4 ±0.129.8 ±0.171.5 ±0.1 Multi SWA61.86 ±0.61.526 ±0.0180.164 ±0.00270.3 ±0.131.3 ±0.171.6 ±0.1 Dropout (SE1)68.49 ±0.181.167 ±0.0010.124 ±0.00277.2 ±0.122.2 ±0.173.5 ±0.1 Table 2. Prediction accuracy (acc), test log-likelihood loss (loss), expected calibration error (ECE), agreement (agr), variance (var) and out-of-domain detection (ODD) for different inference meth- ods trained on the CIFAR datasets. The HMC results are com- puted using the published HMC chains from Ref. [26]. Variances are calculated over independent runs. types of stochastic ensembles are used in our CIFAR',
 '±0.00277.2 ±0.122.2 ±0.173.5 ±0.1 Table 2. Prediction accuracy (acc), test log-likelihood loss (loss), expected calibration error (ECE), agreement (agr), variance (var) and out-of-domain detection (ODD) for different inference meth- ods trained on the CIFAR datasets. The HMC results are com- puted using the published HMC chains from Ref. [26]. Variances are calculated over independent runs. types of stochastic ensembles are used in our CIFAR tests: Monte Carlo dropout (SE1), DropConnect (SE2), and non-parametric dropout (SE3). The drop rates of SE1 and SE2 were tuned to target the HMC results, rather than targeting best performance in terms of accuracy and loss. Each ensemble consists of 50 networks and we use strictly one Monte Carlo inference per ensemble member at test time, i.e. in total 50 inferences per ensemble. This',
 'tests: Monte Carlo dropout (SE1), DropConnect (SE2), and non-parametric dropout (SE3). The drop rates of SE1 and SE2 were tuned to target the HMC results, rather than targeting best performance in terms of accuracy and loss. Each ensemble consists of 50 networks and we use strictly one Monte Carlo inference per ensemble member at test time, i.e. in total 50 inferences per ensemble. This restricts the methods to use the same resources. We also considered increasing the number of inferences per member, obtaining only a minor change in the performance. We report these results in our SM [48]. We also trained multiple realizations of the MultiSWA protocol corresponding to different hyperparameter sets. In the following we only present the closest MultiSWA realization to HMC. For the details on',
 'restricts the methods to use the same resources. We also considered increasing the number of inferences per member, obtaining only a minor change in the performance. We report these results in our SM [48]. We also trained multiple realizations of the MultiSWA protocol corresponding to different hyperparameter sets. In the following we only present the closest MultiSWA realization to HMC. For the details on our hyperparameter selection see the SM [48]. Stochastic ensembles. We found our Monte Carlo dropout ensembles (SE1) to produce more accurate posteriors than DropConnect (SE2) and non-parametric dropout (SE3) ensembles. We therefore present only results corresponding to SE1. We could anticipate SE1 performing better than SE2 and SE3 from the training KL loss analysis. 0 11k3k5kEntropy HMC CIFAR-10 0 1 Regular CIFAR-10 0 1',
 'our hyperparameter selection see the SM [48]. Stochastic ensembles. We found our Monte Carlo dropout ensembles (SE1) to produce more accurate posteriors than DropConnect (SE2) and non-parametric dropout (SE3) ensembles. We therefore present only results corresponding to SE1. We could anticipate SE1 performing better than SE2 and SE3 from the training KL loss analysis. 0 11k3k5kEntropy HMC CIFAR-10 0 1 Regular CIFAR-10 0 1 MultiSWA CIFAR-10 0 1 SE1 DR=0.3 CIFAR-10 0 21k3k5kEntropy HMC CIFAR-100 0 2 Regular CIFAR-100 0 2 MultiSWA CIFAR-100 0 2 SE1 DR=0.2 CIFAR-100Figure 2. Histogram plots of predictive uncertainty computed us- ing different ensemble methods. The HMC baselines are shown in orange in every subplot for better visualisation contrast. 0.4 0.6 0.8 1 Confidence-0.100.1Accuracy - Confidence CIFAR-10 SE1, DR=0.3 MultiSWA Regular HMC 0.2',
 'MultiSWA CIFAR-10 0 1 SE1 DR=0.3 CIFAR-10 0 21k3k5kEntropy HMC CIFAR-100 0 2 Regular CIFAR-100 0 2 MultiSWA CIFAR-100 0 2 SE1 DR=0.2 CIFAR-100Figure 2. Histogram plots of predictive uncertainty computed us- ing different ensemble methods. The HMC baselines are shown in orange in every subplot for better visualisation contrast. 0.4 0.6 0.8 1 Confidence-0.100.1Accuracy - Confidence CIFAR-10 SE1, DR=0.3 MultiSWA Regular HMC 0.2 0.4 0.6 0.8 1 Confidence00.10.2 CIFAR-100 SE1, DR=0.2 MultiSWA Regular HMC Figure 3. Calibration curves associated with different ensembles trained and evaluated on CIFAR. Test performance metrics. In Table 2 we present the test accuracy, test loss, expected calibration error (ECE), agreement, variance, and out-of-distribution detection (ODD) evaluated for different types of ensembles. For metric definitions see our SM [48]. From Table 2 we',
 '0.4 0.6 0.8 1 Confidence00.10.2 CIFAR-100 SE1, DR=0.2 MultiSWA Regular HMC Figure 3. Calibration curves associated with different ensembles trained and evaluated on CIFAR. Test performance metrics. In Table 2 we present the test accuracy, test loss, expected calibration error (ECE), agreement, variance, and out-of-distribution detection (ODD) evaluated for different types of ensembles. For metric definitions see our SM [48]. From Table 2 we read off that the Monte Carlo dropout ensembles (SE1) provide more accurate posteriors than the baseline MultiSWA and regular deep ensembles. Predictive entropy. In Fig. 2 we present the predictive entropy computed for different ensembles trained on CIFAR. SE1 is seen to match more accurately the HMC distribution of entropy values than regular deep ensembles. We also note that MultiSWA and SE1 perform equally',
 'read off that the Monte Carlo dropout ensembles (SE1) provide more accurate posteriors than the baseline MultiSWA and regular deep ensembles. Predictive entropy. In Fig. 2 we present the predictive entropy computed for different ensembles trained on CIFAR. SE1 is seen to match more accurately the HMC distribution of entropy values than regular deep ensembles. We also note that MultiSWA and SE1 perform equally well on CIFAR-10 but SE1 is more accurate on CIFAR-100. Calibration curve. An accurate approximation of Bayesian posterior is expected to be equally calibrated as HMC. In Fig. 3 we depict calibration curves associated with different ensemble methods. All methods are found to be relatively poorly calibrated. The SE1 ensembles are seen to be the closest to HMC. Robustness to distribution shift. To test',
 'well on CIFAR-10 but SE1 is more accurate on CIFAR-100. Calibration curve. An accurate approximation of Bayesian posterior is expected to be equally calibrated as HMC. In Fig. 3 we depict calibration curves associated with different ensemble methods. All methods are found to be relatively poorly calibrated. The SE1 ensembles are seen to be the closest to HMC. Robustness to distribution shift. To test robustness to distribution shifts we evaluate the performance of different methods on the CIFAR-C and CIFAR-100-C datasets [21].',
 'Acc Loss ECE Agr 1e-2Var 1e-2 CIFAR-10-C (mean over corruptions) HMC 3-chains71.05 0.879 0.079 100 0.0 Regular78.48 ±0.210.647 ±0.0060.042 ±0.00179.8 ±0.320.6 ±0.4 Multi SWA77.43 ±0.240.690 ±0.0190.058 ±0.01380.2 ±0.220.9 ±0.2 Dropout (SE1)76.32 ±0.290.713 ±0.0020.064 ±0.00183.1 ±0.116.4 ±0.1 CIFAR-100-C (mean over corruptions) HMC 3-chain42.10 2.610 0.113 100 0.0 Regular49.33 ±0.132.067 ±0.0090.115 ±0.00153.3 ±0.339.7 ±0.2 Multi SWA47.48 ±0.182.153 ±0.0110.112 ±0.00153.4 ±0.239.9 ±0.2 Dropout (SE1)48.35 ±0.052.115 ±0.0030.089 ±0.00161.1 ±0.232.0 ±0.1 Table 3. Performance rates corresponding to different ensemble methods evaluated on CIFAR-C. The CIFAR test datasets are cor- rupted in 16 different ways at various intensities on the scale of 1 to 5. We average the results over all 16·5corrupted datasets and report variances over independent runs. The HMC data was computed using the published HMC chains from Ref. [26]. These datasets',
 '±0.232.0 ±0.1 Table 3. Performance rates corresponding to different ensemble methods evaluated on CIFAR-C. The CIFAR test datasets are cor- rupted in 16 different ways at various intensities on the scale of 1 to 5. We average the results over all 16·5corrupted datasets and report variances over independent runs. The HMC data was computed using the published HMC chains from Ref. [26]. These datasets contain CIFAR images altered using dif- ferent types of corruptions with varying intensities. We use the same 16 corruptions as in Ref. [26]. For each method and corruption we compute the accuracy, loss, ECE, agreement, and variance, and then average over all the corrupted datasets. The corresponding data is presented in Table 3, where SE1 is seen to again be closer to HMC in',
 'contain CIFAR images altered using dif- ferent types of corruptions with varying intensities. We use the same 16 corruptions as in Ref. [26]. For each method and corruption we compute the accuracy, loss, ECE, agreement, and variance, and then average over all the corrupted datasets. The corresponding data is presented in Table 3, where SE1 is seen to again be closer to HMC in terms of the agreement and variance. In Fig. 4 we plot the accuracy and loss resolved over different corruption types and intensities. SE1 is found to be more sensitive to the distribution shifts than the regular ensembles and MultiSWA but still not as much as HMC [26]. Comparison to stochastic gradient Monte Carlo (SGMCMC) approximate posteriors. Our SE1 is closer to HMC than published',
 'terms of the agreement and variance. In Fig. 4 we plot the accuracy and loss resolved over different corruption types and intensities. SE1 is found to be more sensitive to the distribution shifts than the regular ensembles and MultiSWA but still not as much as HMC [26]. Comparison to stochastic gradient Monte Carlo (SGMCMC) approximate posteriors. Our SE1 is closer to HMC than published results [26] for SGMCMC meth- ods in terms of accuracy, loss, ECE, agreement, variance. SE1 also provides closer posteriors to HMC for CIFAR-10- C evaluations, while also being more computationally effi- cient. 1 2 3 4 5 60.40.60.8Accuracy CIFAR-10-C 1 2 3 4 5 60.00.51.01.52.02.5LossHMC Regular MultiSWA SE1, DR=0.3 1 2 3 4 5 6 Corruption Intensity0.20.40.6Accuracy CIFAR-100-C 1 2 3 4 5 6',
 'results [26] for SGMCMC meth- ods in terms of accuracy, loss, ECE, agreement, variance. SE1 also provides closer posteriors to HMC for CIFAR-10- C evaluations, while also being more computationally effi- cient. 1 2 3 4 5 60.40.60.8Accuracy CIFAR-10-C 1 2 3 4 5 60.00.51.01.52.02.5LossHMC Regular MultiSWA SE1, DR=0.3 1 2 3 4 5 6 Corruption Intensity0.20.40.6Accuracy CIFAR-100-C 1 2 3 4 5 6 Corruption Intensity1234567LossHMC Regular MultiSWA SE1, DR=0.2Figure 4. Evaluation on CIFAR-C. The CIFAR test sets are cor- rupted in 16 different ways at various intensities on the scale of 1 to 5. The error bars depict min and max of the corresponding metrics over each corruption, with the boxes indicating quartiles. 7. Conclusions We introduced a variational ansatz for stochastic ensem- bles of neural networks.',
 'Corruption Intensity1234567LossHMC Regular MultiSWA SE1, DR=0.2Figure 4. Evaluation on CIFAR-C. The CIFAR test sets are cor- rupted in 16 different ways at various intensities on the scale of 1 to 5. The error bars depict min and max of the corresponding metrics over each corruption, with the boxes indicating quartiles. 7. Conclusions We introduced a variational ansatz for stochastic ensem- bles of neural networks. Common Bayesian posterior ap- proximations such as regular deep ensembles, Monte Carlo dropout and DropConnect can all be formulated as special cases of these distributional families. Our theoretical and numerical results suggest that the ensembles of stochastic neural networks can provide more accurate approximations to the Bayesian posteriors than other baseline methods. Using our variational ansatz we formulated a new type (SE3) of non-parametric',
 'Common Bayesian posterior ap- proximations such as regular deep ensembles, Monte Carlo dropout and DropConnect can all be formulated as special cases of these distributional families. Our theoretical and numerical results suggest that the ensembles of stochastic neural networks can provide more accurate approximations to the Bayesian posteriors than other baseline methods. Using our variational ansatz we formulated a new type (SE3) of non-parametric stochastic method that can be ap- plied to any layer type with only minor changes to in- corporate parameter sharing between ensemble members. For a simple toy model, we showed with HMC sampling of the Bayesian posterior that our SE3 ensembles provide closer posteriors to HMC than all other considered ensem- ble methods. We evaluated accuracy, loss, ECE, ODD, entropy, agree- ment, variance and',
 'stochastic method that can be ap- plied to any layer type with only minor changes to in- corporate parameter sharing between ensemble members. For a simple toy model, we showed with HMC sampling of the Bayesian posterior that our SE3 ensembles provide closer posteriors to HMC than all other considered ensem- ble methods. We evaluated accuracy, loss, ECE, ODD, entropy, agree- ment, variance and robustness to distribution shift for a ResNet-20-FRN architecture using different types of en- sembles in image classification tasks on CIFAR. We found that our dropout stochastic ensembles (SE1) are closer to the HMC posteriors than all other methods considered. In each of our numeric tests, all the stochastic meth- ods were manually tuned except non-parametric dropout. To understand better what hyperparameters and stochastic method',
 'robustness to distribution shift for a ResNet-20-FRN architecture using different types of en- sembles in image classification tasks on CIFAR. We found that our dropout stochastic ensembles (SE1) are closer to the HMC posteriors than all other methods considered. In each of our numeric tests, all the stochastic meth- ods were manually tuned except non-parametric dropout. To understand better what hyperparameters and stochastic method to use for achieving the most accurate posterior ap- proximations would be highly beneficial. Acknowledgements. OB was supported by the Knut and Alice Wallenberg Foundation and the Swedish Research Council (grant 2017-05162). HL and BM were supported by Vetenskapsr ˚adet (grants 2017-3865 and 2021-4452).',
 'References [1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning: Tech- niques, applications and challenges. Information Fusion , 76:243–297, 2021. [2] David Barber and Christopher M. Bishop. Ensemble learn- ing in bayesian neural networks. In Neural Networks and Machine Learning , pages 215–237. Springer, 1998. [3] David Barber and Charles M. Bishop. Ensemble learning in bayesian neural networks. 1998. [4] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. InProceedings of the 32nd International Conference on In- ternational Conference on Machine Learning - Volume 37 , ICML’15, page 1613–1622. JMLR.org, 2015. [5] Anh Bui, Trung Le, He',
 'Machine Learning , pages 215–237. Springer, 1998. [3] David Barber and Charles M. Bishop. Ensemble learning in bayesian neural networks. 1998. [4] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. InProceedings of the 32nd International Conference on In- ternational Conference on Machine Learning - Volume 37 , ICML’15, page 1613–1622. JMLR.org, 2015. [5] Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas Abraham, and Dinh Phung. Improving Ensemble Robustness by Collaboratively Promoting and Demoting Adversarial Robustness. arXiv e-prints , page arXiv:2009.09612, Sept. 2020. [6] Wray L. Buntine and Andreas S. Weigend. Bayesian back- propagation. Complex Syst. , 5, 1991. [7] Asa Cooper Stickland and Iain Murray. Diverse Ensembles Improve Calibration. arXiv e-prints , page arXiv:2007.04206, July 2020. [8] John S.',
 "Zhao, Paul Montague, Olivier deVel, Tamas Abraham, and Dinh Phung. Improving Ensemble Robustness by Collaboratively Promoting and Demoting Adversarial Robustness. arXiv e-prints , page arXiv:2009.09612, Sept. 2020. [6] Wray L. Buntine and Andreas S. Weigend. Bayesian back- propagation. Complex Syst. , 5, 1991. [7] Asa Cooper Stickland and Iain Murray. Diverse Ensembles Improve Calibration. arXiv e-prints , page arXiv:2007.04206, July 2020. [8] John S. Denker, Daniel B. Schwartz, Ben S. Wittner, Sara A. Solla, Richard E. Howard, Lawrence D. Jackel, and John J. Hopfield. Large automatic learning, rule extraction, and gen- eralization. Complex Syst. , 1, 1987. [9] Francesco D' Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information",
 "Denker, Daniel B. Schwartz, Ben S. Wittner, Sara A. Solla, Richard E. Howard, Lawrence D. Jackel, and John J. Hopfield. Large automatic learning, rule extraction, and gen- eralization. Complex Syst. , 1, 1987. [9] Francesco D' Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems , vol- ume 34, pages 3451–3465. Curran Associates, Inc., 2021. [10] Vikranth Dwaracherla, Zheng Wen, Ian Osband, Xi- uyuan Lu, Seyed Mohammad Asghari, and Benjamin Van Roy. Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping. arXiv e-prints , page arXiv:2206.03633, June 2022. [11] Loic Le Folgoc, Vasileios Baltatzis, Sujal Desai, Anand De- varaj, Sam Ellis, Octavio E. Martinez Manzanera, Arjun",
 'Processing Systems , vol- ume 34, pages 3451–3465. Curran Associates, Inc., 2021. [10] Vikranth Dwaracherla, Zheng Wen, Ian Osband, Xi- uyuan Lu, Seyed Mohammad Asghari, and Benjamin Van Roy. Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping. arXiv e-prints , page arXiv:2206.03633, June 2022. [11] Loic Le Folgoc, Vasileios Baltatzis, Sujal Desai, Anand De- varaj, Sam Ellis, Octavio E. Martinez Manzanera, Arjun Nair, Huaqi Qiu, Julia Schnabel, and Ben Glocker. Is mc dropout bayesian?, 2021. [12] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep Ensembles: A Loss Landscape Perspective. arXiv e- prints , page arXiv:1912.02757, Dec. 2019. [13] Yarin Gal and Zoubin Ghahramani. Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational In- ference. arXiv e-prints , page arXiv:1506.02158, June 2015. [14] Yarin Gal and Zoubin Ghahramani.',
 'Nair, Huaqi Qiu, Julia Schnabel, and Ben Glocker. Is mc dropout bayesian?, 2021. [12] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep Ensembles: A Loss Landscape Perspective. arXiv e- prints , page arXiv:1912.02757, Dec. 2019. [13] Yarin Gal and Zoubin Ghahramani. Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational In- ference. arXiv e-prints , page arXiv:1506.02158, June 2015. [14] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning , volume 48 of Proceedings of MachineLearning Research , pages 1050–1059, New York, New York, USA, 20–22 Jun 2016. PMLR. [15] Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In I. Guyon, U. V',
 'Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning , volume 48 of Proceedings of MachineLearning Research , pages 1050–1059, New York, New York, USA, 20–22 Jun 2016. PMLR. [15] Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fer- gus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. [16] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A Survey of',
 'on Luxburg, S. Bengio, H. Wallach, R. Fer- gus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. [16] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A Survey of Uncertainty in Deep Neu- ral Networks. arXiv e-prints , page arXiv:2107.03342, July 2021. [17] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A survey of uncertainty in deep neural networks, 2022. [18] Alex Graves. Practical variational inference for neural net-',
 'Uncertainty in Deep Neu- ral Networks. arXiv e-prints , page arXiv:2107.03342, July 2021. [17] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A survey of uncertainty in deep neural networks, 2022. [18] Alex Graves. Practical variational inference for neural net- works. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Informa- tion Processing Systems , volume 24. Curran Associates, Inc., 2011. [19] Fredrik K. Gustafsson, Martin Danelljan, and Thomas B. Sch¨on. Evaluating Scalable Bayesian Deep Learning Meth- ods for Robust Computer Vision. arXiv e-prints , page arXiv:1906.01620, June 2019. [20] L. K. Hansen and P. Salamon. Neural',
 'works. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Informa- tion Processing Systems , volume 24. Curran Associates, Inc., 2011. [19] Fredrik K. Gustafsson, Martin Danelljan, and Thomas B. Sch¨on. Evaluating Scalable Bayesian Deep Learning Meth- ods for Robust Computer Vision. arXiv e-prints , page arXiv:1906.01620, June 2019. [20] L. K. Hansen and P. Salamon. Neural network ensembles. IEEE Trans. Pattern Anal. Mach. Intell. , 12(10):993–1001, oct 1990. [21] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. Proceedings of the International Conference on Learn- ing Representations , 2019. [22] Geoffrey E. Hinton and Drew van Camp. Keeping the neu- ral networks simple by minimizing the description length of the weights. In Proceedings',
 'network ensembles. IEEE Trans. Pattern Anal. Mach. Intell. , 12(10):993–1001, oct 1990. [21] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. Proceedings of the International Conference on Learn- ing Representations , 2019. [22] Geoffrey E. Hinton and Drew van Camp. Keeping the neu- ral networks simple by minimizing the description length of the weights. In Proceedings of the Sixth Annual Conference on Computational Learning Theory , COLT ’93, page 5–13, New York, NY , USA, 1993. Association for Computing Ma- chinery. [23] Matthew D. Hoffman and Andrew Gelman. The no-u- turn sampler: Adaptively setting path lengths in hamiltonian monte carlo, 2011. [24] Lara Hoffmann and Clemens Elster. Deep Ensem- bles from a Bayesian Perspective. arXiv e-prints , page arXiv:2105.13283, May',
 'of the Sixth Annual Conference on Computational Learning Theory , COLT ’93, page 5–13, New York, NY , USA, 1993. Association for Computing Ma- chinery. [23] Matthew D. Hoffman and Andrew Gelman. The no-u- turn sampler: Adaptively setting path lengths in hamiltonian monte carlo, 2011. [24] Lara Hoffmann and Clemens Elster. Deep Ensem- bles from a Bayesian Perspective. arXiv e-prints , page arXiv:2105.13283, May 2021. [25] Pavel Izmailov, Dmitrii Podoprikhin, T. Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. ArXiv , abs/1803.05407, 2018. [26] Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, and Andrew Gordon Wilson. What Are Bayesian Neu- ral Network Posteriors Really Like? arXiv e-prints , page arXiv:2104.14421, Apr. 2021. [27] Laurent Valentin Jospin, Hamid Laga, Farid Boussaid,',
 '2021. [25] Pavel Izmailov, Dmitrii Podoprikhin, T. Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. ArXiv , abs/1803.05407, 2018. [26] Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, and Andrew Gordon Wilson. What Are Bayesian Neu- ral Network Posteriors Really Like? arXiv e-prints , page arXiv:2104.14421, Apr. 2021. [27] Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Bennamoun. Hands-on bayesian',
 'neural networks—a tutorial for deep learning users. IEEE Computational Intelligence Magazine , 17(2):29–48, 2022. [28] Diederik P. Kingma, Tim Salimans, and Max Welling. Vari- ational Dropout and the Local Reparameterization Trick. arXiv e-prints , page arXiv:1506.02557, June 2015. [29] Lucas Kook, Andrea G ¨otschi, Philipp FM Baumann, Torsten Hothorn, and Beate Sick. Deep interpretable ensembles. arXiv e-prints , page arXiv:2205.12729, May 2022. [30] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. [31] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive Uncertainty Es- timation using Deep Ensembles. arXiv e-prints , page arXiv:1612.01474, Dec. 2016. [32] Yingzhen Li and Yarin Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. arXiv e-prints , page arXiv:1703.02914, Mar. 2017. [33] H. Linander, O. Balabanov, H. Yang, and',
 'Krizhevsky. Learning multiple layers of features from tiny images. 2009. [31] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive Uncertainty Es- timation using Deep Ensembles. arXiv e-prints , page arXiv:1612.01474, Dec. 2016. [32] Yingzhen Li and Yarin Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. arXiv e-prints , page arXiv:1703.02914, Mar. 2017. [33] H. Linander, O. Balabanov, H. Yang, and B. Mehlig. Look- ing at the posterior: on the origin of uncertainty in neural- network classification, 2022. [34] Christos Louizos and Max Welling. Multiplicative normaliz- ing flows for variational Bayesian neural networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning , volume 70 ofProceedings of Machine Learning Research , pages 2218– 2227. PMLR, 06–11 Aug',
 'B. Mehlig. Look- ing at the posterior: on the origin of uncertainty in neural- network classification, 2022. [34] Christos Louizos and Max Welling. Multiplicative normaliz- ing flows for variational Bayesian neural networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning , volume 70 ofProceedings of Machine Learning Research , pages 2218– 2227. PMLR, 06–11 Aug 2017. [35] David MacKay. Bayesian model comparison and backprop nets. In J. Moody, S. Hanson, and R.P. Lippmann, editors, Advances in Neural Information Processing Systems , vol- ume 4. Morgan-Kaufmann, 1991. [36] D. MacKay. Bayesian Methods for Adaptive Models . PhD thesis, California Institute of Technology, 1992. [37] D. J. C. Mackay. Information-based objective functions for active data selection. Neural Computation , 4(2):550–604, 1992.',
 "2017. [35] David MacKay. Bayesian model comparison and backprop nets. In J. Moody, S. Hanson, and R.P. Lippmann, editors, Advances in Neural Information Processing Systems , vol- ume 4. Morgan-Kaufmann, 1991. [36] D. MacKay. Bayesian Methods for Adaptive Models . PhD thesis, California Institute of Technology, 1992. [37] D. J. C. Mackay. Information-based objective functions for active data selection. Neural Computation , 4(2):550–604, 1992. [38] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. [39] Hendrik Alexander Mehrtens, Camila Gonz ´alez, and Anir- ban Mukhopadhyay.",
 "[38] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. [39] Hendrik Alexander Mehrtens, Camila Gonz ´alez, and Anir- ban Mukhopadhyay. Improving robustness and calibration in ensembles with diversity regularization. arXiv preprint arXiv:2201.10908 , 2022. [40] Aryan Mobiny, Hien V . Nguyen, Supratik Moulik, Naveen Garg, and Carol C. Wu. DropConnect Is Effective in Model- ing Uncertainty of Bayesian Deep Networks. arXiv e-prints , page arXiv:1906.04569, June 2019. [41] Giung Nam, Jongmin Yoon, Yoonho Lee, and Juho Lee. Di- versity Matters When Learning From Ensembles.",
 "Improving robustness and calibration in ensembles with diversity regularization. arXiv preprint arXiv:2201.10908 , 2022. [40] Aryan Mobiny, Hien V . Nguyen, Supratik Moulik, Naveen Garg, and Carol C. Wu. DropConnect Is Effective in Model- ing Uncertainty of Bayesian Deep Networks. arXiv e-prints , page arXiv:1906.04569, June 2019. [41] Giung Nam, Jongmin Yoon, Yoonho Lee, and Juho Lee. Di- versity Matters When Learning From Ensembles. arXiv e- prints , page arXiv:2110.14149, Oct. 2021. [42] Radford M. Neal. Bayesian Learning for Neural Networks . Springer-Verlag, Berlin, Heidelberg, 1996. [43] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshmi- narayanan, and Jasper Snoek. Can you trust your model'suncertainty? evaluating predictive uncertainty under dataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc,",
 "arXiv e- prints , page arXiv:2110.14149, Oct. 2021. [42] Radford M. Neal. Bayesian Learning for Neural Networks . Springer-Verlag, Berlin, Heidelberg, 1996. [43] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshmi- narayanan, and Jasper Snoek. Can you trust your model'suncertainty? evaluating predictive uncertainty under dataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. [44] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B. Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM Comput. Surv. , 54(9), oct 2021. [45] Claude Elwood Shannon. A mathematical theory of commu- nication. The Bell System",
 'E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. [44] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B. Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM Comput. Surv. , 54(9), oct 2021. [45] Claude Elwood Shannon. A mathematical theory of commu- nication. The Bell System Technical Journal , 27:379–423, 1948. [46] Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust bayesian neural networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems , volume 29. Curran Associates, Inc., 2016. [47] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A',
 'Technical Journal , 27:379–423, 1948. [46] Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust bayesian neural networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems , volume 29. Curran Associates, Inc., 2016. [47] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(56):1929–1958, 2014. [48] Supplementary material. [49] Tishby, Levin, and Solla. Consistent inference of probabil- ities in layered networks: predictions and generalizations. InInternational 1989 Joint Conference on Neural Networks , pages 403–409 vol.2, 1989. [50] Aleksei Tiulpin and Matthew B Blaschko. Greedy bayesian posterior approximation with deep ensembles. arXiv preprint arXiv:2105.14275',
 'simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(56):1929–1958, 2014. [48] Supplementary material. [49] Tishby, Levin, and Solla. Consistent inference of probabil- ities in layered networks: predictions and generalizations. InInternational 1989 Joint Conference on Neural Networks , pages 403–409 vol.2, 1989. [50] Aleksei Tiulpin and Matthew B Blaschko. Greedy bayesian posterior approximation with deep ensembles. arXiv preprint arXiv:2105.14275 , 2021. [51] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using drop- connect. In International conference on machine learning , pages 1058–1066. PMLR, 2013. [52] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In ICML , 2011. [53] Yeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: An Alternative Approach to',
 ', 2021. [51] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using drop- connect. In International conference on machine learning , pages 1058–1066. PMLR, 2013. [52] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In ICML , 2011. [53] Yeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning. arXiv e-prints , page arXiv:2002.06715, Feb. 2020. [54] Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. Hyperparameter ensembles for robustness and un- certainty quantification, 2020. [55] Andrew G Wilson and Pavel Izmailov. Bayesian deep learn- ing and a probabilistic perspective of generalization. Ad- vances in neural information processing systems , 33:4697– 4708, 2020. [56] Anqi Wu, Sebastian Nowozin,',
 'Efficient Ensemble and Lifelong Learning. arXiv e-prints , page arXiv:2002.06715, Feb. 2020. [54] Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. Hyperparameter ensembles for robustness and un- certainty quantification, 2020. [55] Andrew G Wilson and Pavel Izmailov. Bayesian deep learn- ing and a probabilistic perspective of generalization. Ad- vances in neural information processing systems , 33:4697– 4708, 2020. [56] Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E Turner, Jose Miguel Hernandez-Lobato, and Alexander L Gaunt. Deterministic variational inference for robust bayesian neural networks. arXiv preprint arXiv:1810.03958 , 2018. [57] Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E. Turner, Jos ´e Miguel Hern ´andez-Lobato, and Alexander L. Gaunt. Deterministic Variational Inference for Ro- bust Bayesian Neural Networks. arXiv e-prints , page arXiv:1810.03958, Oct. 2018.',
 '[58] Sheheryar Zaidi, Arber Zela, Thomas Elsken, Chris C Holmes, Frank Hutter, and Yee Teh. Neural ensemble search for uncertainty estimation and dataset shift. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Process- ing Systems , volume 34, pages 7898–7911. Curran Asso- ciates, Inc., 2021.']